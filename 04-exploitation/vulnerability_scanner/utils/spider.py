"""Website spidering utilities"""

import re
from urllib.parse import urljoin, urlparse
from ..config import MAX_SPIDER_DEPTH

class Spider:
    """Web spider to find testable URLs"""
    
    def __init__(self, request_handler, target_url):
        self.request_handler = request_handler
        self.target_url = target_url
        self.target_domain = urlparse(target_url).netloc
    
    def spider_website(self, max_depth=MAX_SPIDER_DEPTH):
        """
        Spider website to find URLs with parameters
        Returns: List of URLs with query parameters
        """
        print(f"[*] Spidering website to find testable URLs...")
        
        visited = set()
        to_visit = {self.target_url}
        found_urls = []
        
        depth = 0
        while to_visit and depth < max_depth:
            current_urls = to_visit.copy()
            to_visit.clear()
            
            for url in current_urls:
                if url in visited:
                    continue
                    
                visited.add(url)
                
                response = self.request_handler.get(url)
                if not response:
                    continue
                
                # Store URLs with parameters for testing
                if '?' in url and '=' in url:
                    found_urls.append(url)
                
                # Find more links
                links = re.findall(r'href=["\']([^"\']+)["\']', response.text)
                
                for link in links:
                    absolute_url = urljoin(url, link)
                    
                    # Only follow links on same domain
                    if urlparse(absolute_url).netloc == self.target_domain:
                        if absolute_url not in visited:
                            to_visit.add(absolute_url)
            
            depth += 1
        
        print(f"    [+] Found {len(found_urls)} testable URLs")
        return found_urls